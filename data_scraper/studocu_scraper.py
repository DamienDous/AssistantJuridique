import csv, os, time
from glob import glob
from pathlib import Path
import numpy as np
import random
import cv2
import undetected_chromedriver as uc
from bs4 import BeautifulSoup
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
import unicodedata
import subprocess
import re
from selenium.webdriver.common.keys import Keys
from io import BytesIO
from PIL import Image
from urllib.parse import urlparse


TEMP_FOLDER = "dossier_temp"
SCORE_THRESHOLD = 0.4
SCROLL_OFFSET = 835
# Journal de scraping CSV
log_entries = []

def studocu_slug(url):
    # 1. Extraire le chemin de l'URL
    path = urlparse(url).path
    # 2. Supprimer le premier / (toujours pr√©sent)
    path = path.lstrip('/')
    # 3. Enlever l'ID final (toujours num√©rique, avant √©ventuel "?")
    parts = path.split('/')
    # On enl√®ve le dernier √©l√©ment s'il ne contient que des chiffres
    if parts and parts[-1].isdigit():
        parts = parts[:-1]
    # 4. Joindre tous les morceaux avec un tiret
    slug = '-'.join(parts)
    return slug

def init_driver():
	options = uc.ChromeOptions()
	options.add_argument("--no-sandbox")
	options.add_argument("--disable-dev-shm-usage")
	options.add_argument("--disable-blink-features=AutomationControlled")
	options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
						 "AppleWebKit/537.36 (KHTML, like Gecko) "
						 "Chrome/123.0.0.0 Safari/537.36")

	print(f"Initialisation du driver Chrome en mode 'visuel'‚Ä¶")
	driver = uc.Chrome(service=Service(ChromeDriverManager().install()), options=options)

	# Masquer navigator.webdriver
	driver.execute_cdp_cmd(
		"Page.addScriptToEvaluateOnNewDocument",
		{
			"source": """
				Object.defineProperty(navigator, 'webdriver', {
					get: () => undefined
				});
			"""
		},
	)

	return driver

def recherche_studocu(driver, mot_cle):
    print(f"üîç Test de recherche pour : {mot_cle}")
    driver.get("https://www.studocu.com/fr/")

    time.sleep(2.5)
    # Cookies
    try:
        WebDriverWait(driver, 6).until(
            EC.element_to_be_clickable((By.XPATH, "//button[contains(text(), 'Tout refuser')]"))
        ).click()
        print("‚úÖ Cookies refus√©s.")
    except:
        print("‚ÑπÔ∏è Pas de popup cookies d√©tect√©.")

    all_liens = set()  # Pour √©viter les doublons

    try:
        # Recherche du champ
        champ = WebDriverWait(driver, 10).until(
            EC.element_to_be_clickable((By.CSS_SELECTOR, "input[placeholder*='Rechercher']"))
        )
        for tentative in range(2):
            champ.click()
            time.sleep(0.5)
            champ.clear()
            champ.send_keys(mot_cle)
            time.sleep(0.5)
            if champ.get_attribute("value").strip():
                break
            print("‚è≥ Le mot-cl√© n‚Äôa pas √©t√© ins√©r√©, nouvelle tentative‚Ä¶")
        
        champ.submit()
        print("‚úÖ Requ√™te envoy√©e.")

        time.sleep(2)
        page_num = 1
        while True:
            # Attendre chargement des liens
            WebDriverWait(driver, 10).until(
                EC.presence_of_all_elements_located((By.CSS_SELECTOR, 'a[href*="/fr/document/"]'))
            )

            # R√©cup√©rer les liens de la page courante
            liens = driver.find_elements(By.CSS_SELECTOR, 'a[href*="/fr/document/"]')
            nb_avant = len(all_liens)
            for l in liens:
                href = l.get_attribute("href")
                if href and "/fr/document/" in href:
                    all_liens.add(href)
            print(f"üü¢ Page {page_num} : {len(all_liens) - nb_avant} nouveaux liens")

            # Chercher bouton "Suivant"
            try:
                next_btn = driver.find_element(By.CSS_SELECTOR, 'button[data-test-selector="search-document-pagination-next-button"]')
                if next_btn.get_attribute("disabled"):
                    print("‚èπÔ∏è Plus de pages suivantes.")
                    break  # Bouton d√©sactiv√© : fin des pages
                next_btn.click()
                page_num += 1
                time.sleep(2.2)  # Attendre chargement nouvelle page
            except Exception as e:
                print("‚èπÔ∏è Bouton 'Suivant' introuvable ou erreur :", e)
                break

        print(f"üü¢ Total {len(all_liens)} liens r√©cup√©r√©s pour la recherche '{mot_cle}'")
        for lien in list(all_liens)[:5]:
            print(" ‚ûú", lien)

        return list(all_liens)

    except Exception as e:
        print("‚ùå Erreur pendant la recherche :", e)
        return []

def recherche_multi_studocu(driver, requetes, csv_output):
	liens_total = []
	for requete in requetes:
		liens = recherche_studocu(driver, requete)
		for url in liens:  # ‚õî Limitation √† 2 liens max par mot-cl√©
			liens_total.append({
				"requete": requete,
				"url": url
			})
			
		time.sleep(random.uniform(2, 4))  # petite pause entre les requ√™tes

	with open(csv_output, "w", encoding="utf-8-sig", newline='') as f:
		writer = csv.DictWriter(f, fieldnames=["requete", "url"], delimiter=";")
		writer.writeheader()
		writer.writerows(liens_total)

	print(f"‚úÖ Fichier CSV enregistr√© : {csv_output} ({len(liens_total)} liens)")

def nettoyer_texte(html):
	soup = BeautifulSoup(html, "html.parser")
	for script in soup(["script", "style", "noscript"]):
		script.decompose()
	texte = soup.get_text(separator="\n")
	lignes = texte.splitlines()
	propre = []
	for ligne in lignes:
		ligne = unicodedata.normalize("NFKC", ligne.strip())
		if len(ligne) >= 25 and not ligne.lower().startswith("t√©l√©charger") and "studocu" not in ligne.lower():
			propre.append(" ".join(ligne.split()))
	return "\n".join(propre)

def scrape_contenu_premium_depuis_csv(driver, csv_path, dossier_sortie):
	if not os.path.exists(dossier_sortie):
		os.makedirs(dossier_sortie)

	with open(csv_path, "r", encoding="utf-8-sig") as f:
		reader = csv.DictReader(f, delimiter=";")
		for ligne in reader:
			url = ligne["url"]
			titre = ligne["titre"]
			print(f"\nüîó Ouverture : {url}")
			try:
				driver.get(url)

				WebDriverWait(driver, 10).until(
					EC.presence_of_element_located((By.TAG_NAME, "body"))
				)
				time.sleep(3.5)  # attendre chargement PDF/HTML

				# V√©rifie si le document est premium (aper√ßu)
				try:
					driver.find_element(By.XPATH, "//div[contains(text(), 'Ceci est un aper√ßu')]")
					print("‚ö†Ô∏è Document restreint (aper√ßu seulement)")
				except:
					print("‚úÖ Document complet visible")

				# R√©cup√©ration du contenu visible
				html = driver.page_source
				texte = nettoyer_texte(html)

				if len(texte) > 500:
					nom_fichier = titre.replace("/", "-").replace(":", "-").replace("?", "").strip()
					nom_fichier = "_".join(nom_fichier.split())[:100] + ".txt"
					chemin_fichier = os.path.join(dossier_sortie, nom_fichier)
					with open(chemin_fichier, "w", encoding="utf-8") as out:
						out.write(texte)
					print(f"üíæ Texte sauvegard√© : {chemin_fichier}")
				else:
					print("‚õî Texte trop court ou non r√©cup√©r√©.")

			except Exception as e:
				print(f"‚ùå √âchec sur {url} : {e}")

def login_studocu(driver, email, mot_de_passe):
	driver.get("https://www.studocu.com/fr/login/")

	try:
		WebDriverWait(driver, 6).until(
			EC.element_to_be_clickable((By.XPATH, "//button[contains(text(), 'Tout refuser')]"))
		).click()
		print("‚úÖ Cookies refus√©s.")
	except:
		print("‚ÑπÔ∏è Pas de pop-up cookies d√©tect√©.")

	try:
		bouton_email = WebDriverWait(driver, 8).until(
			EC.element_to_be_clickable((By.CSS_SELECTOR, "button[data-test-selector='email-login-button']"))
		)
		driver.execute_script("arguments[0].click();", bouton_email)
		print("‚úÖ 'Continuer avec un e-mail' cliqu√© via JS.")
	except Exception as e:
		print(f"‚ùå √âchec clic 'Continuer avec un e-mail' : {e}")
		return

	try:
		champ_email = WebDriverWait(driver, 8).until(EC.presence_of_element_located((By.NAME, "email")))
		champ_email.clear()
		champ_email.send_keys(email)
		time.sleep(0.3)

		champ_mdp = driver.find_element(By.NAME, "password")
		champ_mdp.clear()
		champ_mdp.send_keys(mot_de_passe)
		time.sleep(0.5)

		# ‚úÖ Simulation humaine : touche Entr√©e
		champ_mdp.send_keys(Keys.ENTER)
		print("üîê Connexion envoy√©e via touche Entr√©e.")

		WebDriverWait(driver, 10).until(
			EC.presence_of_element_located((By.XPATH, "//a[contains(@href, '/fr/')]"))
		)
		print("‚úÖ Connect√© √† Studocu.")

	except Exception as e:
		print(f"‚ùå √âchec de la connexion : {e}")
	
def capture_vue_premiere_page(driver, url, dossier):

	time.sleep(4)
	url_finale = driver.current_url
	print(f"üìç URL apr√®s chargement : {url_finale}")

	with open(os.path.join(dossier_temp, "debug_page_source.html"), "w", encoding="utf-8") as f:
		f.write(driver.page_source)
	print("üß© HTML sauvegard√© pour inspection.")

	# üîé Zoom (facultatif)
	try:
		driver.execute_script("document.body.style.zoom='61%'")
		time.sleep(1.5)
	except Exception as e:
		print(f"‚ö†Ô∏è Zoom √©chou√© : {e}")

	# üß≠ Scroll progressif sur le conteneur scrollable
	try:
		print("üîç Recherche du conteneur scrollable‚Ä¶")
		scrollable = driver.find_element(By.ID, "document-wrapper")
		scroll_height = driver.execute_script("return arguments[0].scrollHeight", scrollable)
		client_height = driver.execute_script("return arguments[0].clientHeight", scrollable)
		SCROLL_OFFSET = int(client_height / 2)
		print(SCROLL_OFFSET)
		nb_scrolls = max(1, 2 * scroll_height // client_height)  # double de captures
		print(f"üìè scrollHeight = {scroll_height}, clientHeight = {client_height}")
		print(f"üì∏ Nombre de scrolls estim√© : {nb_scrolls}")

		for i in range(nb_scrolls):
			driver.execute_script(
				"arguments[0].scrollTop = (arguments[0].clientHeight / 2) * arguments[1];",
				scrollable, i
			)
			time.sleep(1.2)
			image = Image.open(BytesIO(driver.get_screenshot_as_png()))
			image_path = os.path.join(dossier_temp, f"{titre}_vue{i+1}.png")
			image.save(image_path)
			print(f"‚úÖ Capture {i+1}/{nb_scrolls} enregistr√©e : {image_path}")
	except Exception as e:
		print(f"‚ùå Scroll ou capture √©chou√©e : {e}")

	return dossier_temp

def natural_sort_key(s):
	return [int(text) if text.isdigit() else text.lower() for text in re.split(r'(\d+)', s)]

def decouper_image_zone_utilisable(image: np.ndarray) -> np.ndarray:
	# Zone utile : ajuste selon tes besoins
	x1, y1 = 1380, 200
	x2, y2 = 2620, 1800
	return image[y1:y2, x1:x2]

def zone_difference(img1, img2, template_path, max_offset=300):
	h = min(img1.shape[0], img2.shape[0])
	min_score = float("inf")
	best_y = 0

	# Recherche classique du meilleur chevauchement
	for y in range(20, max_offset):
		patch1 = img1[-y:]
		patch2 = img2[:y]
		if patch1.shape[0] != patch2.shape[0]:
			continue
		diff = np.abs(patch1.astype(np.int16) - patch2.astype(np.int16))
		score = np.mean(diff)
		if score < min_score:
			min_score = score
			best_y = y

	print(f"üîç Meilleur chevauchement initial √† y = {best_y} (score={min_score})")

	return best_y

def detect_popup_bbox(res, template_shape, threshold=0.7):
	ys, xs = np.where(res >= threshold)
	if len(xs) == 0:
		return None
	y_popup = np.max(ys)
	xs_popup = xs[ys == y_popup]
	if len(xs_popup) == 0:
		return None
	x_min = np.min(xs_popup)
	h_t, w_t = template_shape
	# On prend le x_min, le y_popup, et on rajoute la largeur/hauteur du template
	return (x_min, y_popup, x_min + w_t, y_popup + h_t)

def remplacer_popup_par_patch_suivant(images_utiles, y_cuts, template_path, debug_dir):
	os.makedirs(debug_dir, exist_ok=True)
	template = cv2.imread(template_path)
	if template is None:
		print(f"‚ùå Template non trouv√© : {template_path}")
		return

	h_t, w_t = template.shape[:2]
	nb_patched = 0

	# Parcourir chaque paire d'images
	for i in range(len(images_utiles) - 1):
		img = images_utiles[i]
		img_suiv = images_utiles[i + 1]
		y_cut = y_cuts[i]

		# D√©tection pr√©cise des popups
		res = cv2.matchTemplate(img, template, cv2.TM_CCOEFF_NORMED)
		threshold = SCORE_THRESHOLD  # Utilise la variable globale (ou passe-la en argument)
		bbox = detect_popup_bbox(res, template.shape[:2], threshold)
		# ... d√©tection popup ...
		if bbox:
			x1, y1, x2, y2 = bbox
			print("bbox : ", x1, y1, x2, y2)
			print("y_cut : ", y_cut)

			y1_suiv = y1 - SCROLL_OFFSET
			y2_suiv = y2 - SCROLL_OFFSET

			print("y1_suiv - y2_suiv: ", y1_suiv, y2_suiv)
			# V√©rifie que √ßa reste dans les limites
			if y1_suiv < 0: y1_suiv = 0
			if y2_suiv > img_suiv.shape[0]: y2_suiv = img_suiv.shape[0]
			patch_propre = img_suiv[y1_suiv:y2_suiv, x1:x2]
			if patch_propre.shape == (y2-y1, x2-x1, 3):
				img[y1:y2, x1:x2] = patch_propre
				nb_patched += 1
				cv2.imwrite(f"{debug_dir}/debug_popup_patch_img{i}_{x1}_{y1}.png", patch_propre)
			else:
				print(f"Patch incorrect sur img {i+1}: {patch_propre.shape} au lieu de {(y2-y1, x2-x1, 3)}")

	if nb_patched == 0:
		print("‚ùó Aucun popup remplac√© sur cette s√©rie.")
	else:
		print(f"‚úÖ {nb_patched} popup(s) pr√©cis√©ment remplac√©(s).")

def assembler_document(dossier, sortie):
	
	# 1. Recherche tous les fichiers image (_vue*.png) dans le dossier et trie selon l'ordre naturel
	# Ouvre chaque image (format OpenCV) et stocke dans une liste
	chemins = sorted(glob(os.path.join(dossier, "*_vue*.png")), key=natural_sort_key)
	images = [cv2.imread(p) for p in chemins]
	images_utiles = []

	# 2. D√©coupe chaque image pour ne garder que la "zone utile" (zone centrale du document)
	for i, img in enumerate(images):
		decoupee = decouper_image_zone_utilisable(img)
		images_utiles.append(decoupee)
		debug_path = os.path.join(dossier, f"decoupe_debug_{i+1:02d}.png")
		cv2.imwrite(debug_path, decoupee)
		print(f"üß™ Image d√©coup√©e enregistr√©e : {debug_path} ({decoupee.shape[1]}x{decoupee.shape[0]})")

	# 3. Pour chaque paire d'images cons√©cutives, calcule la hauteur de chevauchement optimale (y_cut)
	# Cela permet de savoir √† partir de quelle ligne il faut "assembler" l'image suivante
	y_cuts = []
	for i in range(len(images_utiles) - 1):
		y_cut = zone_difference(
			images_utiles[i],
			images_utiles[i+1],
			template_path=os.path.join(os.getcwd(), "popup.png"),
			max_offset=images_utiles[i].shape[0] // 2
		)
		y_cuts.append(y_cut)

	# 4. Supprime les popups si besoin :
	#    Pour chaque image (sauf la derni√®re), d√©tecte un √©ventuel popup et le remplace
	#    par un patch pris √† la m√™me position dans l'image suivante, en tenant compte du scroll (y_cut)
	remplacer_popup_par_patch_suivant(images_utiles, y_cuts, template_path=os.path.join(os.getcwd(), "popup.png"), debug_dir="debug_patches")

	# 5. Reconstruit le document fusionn√©¬†:
	#    Assemble les images en "coupant" les zones de recouvrement d√©j√† utilis√©es,
	#    et en les collant √† la suite les unes des autres.
	h_img, w_img = images_utiles[0].shape[:2]
	segments = [images_utiles[0]]
	for i in range(1, len(images_utiles)):
		y_cut = y_cuts[i-1]  # R√âUTILISE la valeur d√©j√† calcul√©e
		print(f"üîé Image {i+1}: d√©coupage dynamique √† y = {y_cut}")
		segments.append(images_utiles[i][y_cut:])

	# 6. Construit un grand canvas final, colle tous les segments √† la suite verticalement
	h_total = sum(seg.shape[0] for seg in segments)
	canvas = np.zeros((h_total, w_img, 3), dtype=np.uint8)

	y_offset = 0
	for seg in segments:
		h = seg.shape[0]
		canvas[y_offset:y_offset + h, :w_img] = seg
		y_offset += h

	# 7. Enregistre l'image fusionn√©e finale
	cv2.imwrite(sortie, canvas)
	print(f"‚úÖ Document final fusionn√© enregistr√© sous : {sortie}")


if __name__ == "__main__":
	# Initialiser le driver
	driver = init_driver()
	try:
		email = "damien.dous@gmail.com"
		mot_de_passe = "azerty1!"
		login_studocu(driver, email, mot_de_passe)
		time.sleep(3)  # ‚è≥ Attendre que la session soit bien active
		
		# os.makedirs(TEMP_FOLDER, exist_ok=True)
		# csv_output = "studocu_liens.csv"
		# requetes=["cas pratique droit", "exercice pratique droit"]
		# recherche_multi_studocu(driver, requetes, csv_output)

		with open("studocu_liens.csv", encoding="utf-8-sig") as f:
			reader = csv.DictReader(f, delimiter=";")
			for ligne in reader:
				print(ligne["url"])
				url = ligne["url"]
				titre_brut = ligne["url"]
				# titre = titre_brut.replace(" ", "_").replace("/", "_").replace(":", "_").replace("?", "").replace("\"", "").strip()
				titre = studocu_slug(titre_brut)  # limite pour √©viter des noms trop longs
				dossier_temp = TEMP_FOLDER+"/"+titre+"_captures_debug"
				os.makedirs(dossier_temp, exist_ok=True)

				print(f"\nüîó Acc√®s au document premium : {url}")
				driver.get(url)

				capture_vue_premiere_page(driver=driver, url=url, dossier=dossier_temp)
				assembler_document(dossier=dossier_temp, sortie=dossier_temp+"_document_fusionne_final.png")

	finally:
		driver.quit()
	# fusionner_captures_verticales(dossier="captures_debug")



